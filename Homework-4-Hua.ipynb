{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Homework-4-Hua.ipynb","provenance":[{"file_id":"1HLcDmDyyE4iA-TSmWO__cnk0BE6nP7_l","timestamp":1643840536006},{"file_id":"14OuGGEH1O0jPymy7FvVYzl4D94zg4aj2","timestamp":1643822145333}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. Backpropagation for bias vectors (1 point)\n","\n","In class, we discussed a multilayer perceptron (neural network) whose layers were all \"dense\", i.e. the output $a^m \\in \\mathbb{R}^{N^m}$ of the $m$th layer is computed as \n","\\begin{align*}\n","z^m &= W^m a^{m - 1} + b^m \\\\\n","a^m &= \\sigma^m(z^m)\n","\\end{align*}\n","where $W^m \\in \\mathbb{R}^{N^m \\times N^{m - 1}}$ is the weight matrix, $b^m \\in \\mathbb{R}^{N^m}$ is the bias vector, and $\\sigma^m$ is the nonlinearity. We showed that \n","$$\\frac{\\partial C}{\\partial W^m} = \\frac{\\partial C}{\\partial z^m} a^{m - 1 \\top}$$\n","Show that\n","$$\\frac{\\partial C}{\\partial b^m} = \\frac{\\partial C}{\\partial z^m}$$\n","Hint: The derivation is almost the same as for $W$."],"metadata":{"id":"nkK2eaK1SiNt"}},{"cell_type":"markdown","source":["Proof: $$\\frac{\\partial C}{\\partial b^m} = \\sum_{k=1}^{N}\\frac{\\partial C}{\\partial z^m_{k}} \\frac{\\partial z^m_{k}}{\\partial b^m_{ij}}$$\n","\n","When i=k \n","\\begin{align*}\n","\\frac{\\partial z_{i}^{m}}{\\partial b^m_{ij}} = \n","\\frac{\\partial}{\\partial b^m_{ij}} (\\sum_{l=1}^{N}W_{il}^{m} a_{l}^{m-1}+b_i^m)=\\vec1\n","\\end{align*}\n","\n","Otherwise, $$\\frac{\\partial z_{i}^{m}}{\\partial b^m_{ij}} =0$$\n","\n","Thus, $$\\frac{\\partial C}{\\partial b^m_{ij}} = \\frac{\\partial C}{\\partial z^m_{i}} \\cdot \\vec1$$\n","\n","Using the matrix notation, $$\\frac{\\partial C}{\\partial b^m} = \\frac{\\partial C}{\\partial z^m}$$"],"metadata":{"id":"DbZ6cXowl53T"}},{"cell_type":"markdown","source":["# 2. MLP from scratch (3 points)\n","\n","Using numpy only, implement backward pass or a sigmoid MLP. Specifically, you will need to implement this functionality in the `train` function in the `SigmoidMLP` class below. You should write numpy code to populate the two lists `weight_gradients` and `bias_gradient`, where each entry in each list corresponds to the gradient for a weight matrix or bias vector for each layer. Then, when you run the code cell at the bottom of this notebook, the trained MLP should output (approximately) 0, 1, 1, 0, having learned the [XOR function](https://en.wikipedia.org/wiki/Exclusive_or). Please us a binary cross-entropy loss, i.e.\n","$$C(a^L, y) = (y - 1)\\log(1 - a^L) - y\\log(a^L)$$\n","\n","*Note 1*: All layers in your model, including the last layer, will use the sigmoid cross-entropy function. Remember that \n","$$\n","\\frac{\\partial}{\\partial x}\\mathrm{sigmoid}(x) = \\mathrm{sigmoid}(x)(1 - \\mathrm{sigmoid}(x))$$\n","\n","*Note 2*: As we mentioned in class,\n","$$\n","\\frac{\\partial C}{\\partial z^L} = a^L - y\n","$$"],"metadata":{"id":"TerrZtS6T31p"}},{"cell_type":"code","source":["import numpy as np\n","def sigmoid(x):\n","  \n","    z = np.exp(-x)\n","    sig = 1 / (1 + z)\n","\n","    return sig"],"metadata":{"id":"A7dHCFrIrw3a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class Layer:\n","    def __init__(self, inputs, outputs):\n","        # Initialize weight matrix and bias vector\n","        # Getting the initialization right can be tricky, but for this problem\n","        # simply drawing from a standard normal distribution should work.\n","        self.weights = np.random.randn(outputs, inputs)\n","        self.biases = np.random.randn(outputs, 1)\n","    def __call__(self, X):\n","        # Compute \\sigmoid(Wx + b)\n","        return 1/(1 + np.exp(-(self.weights.dot(X) + self.biases)))\n","\n","class SigmoidMLP:\n","\n","    def __init__(self, layer_widths):\n","        self.layers = []\n","        for inputs, outputs in zip(layer_widths[:-1], layer_widths[1:]):\n","            self.layers.append(Layer(inputs, outputs))\n","    \n","    def train(self, inputs, targets, learning_rate):\n","        # Forward pass - compute each layer's output and store it for later use\n","        layer_outputs = [inputs]\n","        for layer in self.layers:\n","            layer_outputs.append(layer(layer_outputs[-1]))\n","        \n","        # Implement backward pass to populate weight_gradients and bias_gradients\n","        # lists here\n","        weight_gradients = []\n","        bias_gradients = []\n","        # ... (your code here) ...\n","        a2 = layer_outputs[-1]\n","        a1 = layer_outputs[-2]\n","        a0 = layer_outputs[-3]\n","        W1 = self.layers[0].weights\n","        b1 = self.layers[0].biases\n","        Z1 = np.dot(W1,a0) + b1\n","        W2 = self.layers[1].weights\n","        cost = (targets-1)*np.log(1-a2) - targets*np.log(a2)\n","        da2 = -targets/a2 + (1-targets)/(1-a2)\n","        dZ2 = a2-targets\n","        dW2 = np.dot(dZ2,a1.T)\n","        db2 = np.sum(dZ2, axis = 1, keepdims = True)\n","        da1 = np.dot(W2.T,dZ2)\n","        dZ1 = da1*(sigmoid(Z1)*(1-sigmoid(Z1)))\n","        dW1 = np.dot(dZ1,a0.T)\n","        db1 = np.sum(dZ1, axis = 1, keepdims = True)\n","        weight_gradients.append(dW1)\n","        weight_gradients.append(dW2)\n","        bias_gradients.append(db1)\n","        bias_gradients.append(db2)\n","\n","        # Perform gradient descent by applying updates\n","        for weight_gradient, bias_gradient, layer in zip(weight_gradients, bias_gradients, self.layers):\n","            layer.weights -= weight_gradient*learning_rate\n","            layer.biases -= bias_gradient*learning_rate\n","\n","    def __call__(self, inputs):\n","        a = inputs\n","        for layer in self.layers:\n","            a = layer(a)\n","        return a\n","\n","def train_mlp(n_iterations, learning_rate):\n","    mlp = SigmoidMLP([2, 2, 1])\n","    inputs = np.array([[0, 1, 0, 1], \n","                       [0, 0, 1, 1]])\n","    targets = np.array([[0, 1, 1, 0]])\n","    for _ in range(int(1e3)):\n","        mlp.train(inputs, targets, learning_rate)\n","    return mlp"],"metadata":{"id":"W-v0EeRsV8m9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# You may need to change the n_iterations and learning_rate values\n","# but these worked for me\n","mlp = train_mlp(1000, 1)\n","# The following calls should result in (approximately) 0, 1, 1, 0\n","# If the outputs are somewhat close, your training has succeeded!\n","print(mlp(np.array([0, 0]).reshape(-1, 1)))\n","print(mlp(np.array([0, 1]).reshape(-1, 1)))\n","print(mlp(np.array([1, 0]).reshape(-1, 1)))\n","print(mlp(np.array([1, 1]).reshape(-1, 1)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yoAl7lW_WyNg","executionInfo":{"status":"ok","timestamp":1644418289025,"user_tz":300,"elapsed":527,"user":{"displayName":"Hanqi Hua","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06347690475984403200"}},"outputId":"e5015712-3030-4274-a4c4-829cedcee6a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.00247338]]\n","[[0.99741727]]\n","[[0.99741766]]\n","[[0.00386548]]\n"]}]}]}